# Video Processing Integration

## Overview
The video processing stage captures webcam footage while showing an emotionally triggering video, then processes each frame through 4 ML models in real-time.

## Architecture

### Backend (`backend/main.py`)
- **POST /api/video/start-session**: Initialize VideoAnalyzer and reset state
- **POST /api/video/process-frame**: Process single frame through all models
- **POST /api/video/submit**: Save complete timeline and summary to JSONL

### Frontend (`frontend/video.js`)
1. Request webcam access
2. Start backend session
3. Play trigger video (`hack.mp4`)
4. Capture frames at 30 FPS
5. Send each frame to backend for processing
6. Display live stats (blinks, emotion, gaze)
7. On video end, submit complete results

### Processors (`processors/video_analyzer.py`)
Unified analyzer integrating:
- **Emotion**: 3 FPS (every 10 frames) - Transformers ViT
- **Blink**: 15 FPS (every 2 frames) - MediaPipe Face Mesh
- **Iris/Pupil**: 15 FPS (every 2 frames) - MediaPipe Iris
- **Gaze**: 10 FPS (every 3 frames) - L2CS-Net

## Data Flow

```
Webcam Frame (640x480 @ 30fps)
    ↓
Frontend: Capture & Convert to Base64
    ↓
POST /api/video/process-frame
    ↓
Backend: Decode & Convert to OpenCV format
    ↓
VideoAnalyzer.process_frame()
    ├─→ EmotionAnalyzer (if frame_num % 10 == 0)
    ├─→ BlinkDetector (if frame_num % 2 == 0)
    ├─→ IrisTracker (if frame_num % 2 == 0)
    └─→ L2CS Pipeline (if frame_num % 3 == 0)
    ↓
Return JSON result to frontend
    ↓
Frontend: Update live stats & timeline
    ↓
Video ends → POST /api/video/submit
    ↓
Save to backend/data/video_results.jsonl
```

## Timeline Data Structure

Each frame entry in timeline:
```json
{
  "timestamp": 1.5,
  "frame_number": 45,
  "processed": {
    "emotion": true,
    "blink": false,
    "iris": false,
    "gaze": true
  },
  "emotion": {
    "label": "sad",
    "confidence": 0.85,
    "scores": {"sad": 0.85, "neutral": 0.10, ...}
  },
  "blink": {
    "ear_left": 0.25,
    "ear_right": 0.26,
    "avg_ear": 0.255,
    "is_blinking": false,
    "cumulative_blinks": 12
  },
  "pupil": {
    "left": 0.032,
    "right": 0.031,
    "avg": 0.0315,
    "dilation_ratio": 0.05
  },
  "gaze": {
    "pitch": -5.2,
    "yaw": 12.3,
    "direction": "center"
  }
}
```

## Summary Statistics

Generated by `VideoAnalyzer.get_summary()`:
```json
{
  "duration_seconds": 30.0,
  "total_frames": 900,
  "emotion": {
    "distribution": {"sad": 15, "neutral": 10, "fear": 5},
    "dominant_emotion": "sad",
    "emotion_changes": 8
  },
  "blink": {
    "total_blinks": 18,
    "blink_rate_per_minute": 36.0
  },
  "pupil": {
    "avg_pupil_size": 0.0315,
    "max_pupil_size": 0.0425,
    "min_pupil_size": 0.0255,
    "pupil_dilation_events": 12
  },
  "gaze": {
    "distribution_percentage": {
      "left": 15.5,
      "center": 72.3,
      "right": 12.2
    },
    "attention_score": 0.723
  }
}
```

## Key Features

1. **Real-time Processing**: Frames processed as they're captured
2. **Optimized Sampling**: Each model runs at appropriate FPS
3. **Live Feedback**: User sees blinks, emotions, gaze in real-time
4. **Complete Timeline**: All data preserved for visualization
5. **Server-side Summary**: Authoritative statistics from backend
6. **Session Persistence**: Results saved to JSONL with session ID

## Performance Notes

- Frame capture: 30 FPS (WebRTC ImageCapture API)
- Backend processing: ~50-150ms per frame (depending on sampling)
- Network latency: Minimal (localhost)
- Total processing time: ~30-45 seconds for 30-second video

## Dependencies

- **Frontend**: getUserMedia, ImageCapture API, Canvas API
- **Backend**: FastAPI, OpenCV, NumPy, Base64
- **Models**: Transformers, MediaPipe, L2CS-Net, PyTorch

## Testing

Run individual model tests:
```bash
python tests/test_blink_detection.py
python tests/test_iris_tracking.py
python tests/test_l2cs_gaze.py
```

## Future Enhancements

- [ ] Batch frame processing (send 5-10 frames at once)
- [ ] WebSocket for real-time streaming
- [ ] GPU acceleration for faster processing
- [ ] Emotion intensity scoring
- [ ] Micro-expression detection
